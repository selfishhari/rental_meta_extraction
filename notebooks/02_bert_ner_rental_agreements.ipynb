{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "02_bert_ner_rental_agreements.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ysLj3Z7veGV",
        "colab_type": "text"
      },
      "source": [
        "**Objective:**\n",
        "\n",
        "Use BERT Tokenizer to build a NER Tagger for Party Recognition from rental agreements\n",
        "\n",
        "**Results:**\n",
        "\n",
        "80% + recall for Party1/Party2 tags on provided validation set\n",
        "\n",
        "**Contents:**\n",
        "\n",
        "\n",
        "1.   Read the BIO annotated data\n",
        "2.   Undersample paragraphs without any tags in them to have a ratio of 1:1\n",
        "3. Determine Max Length of the words in each para/text and truncate/add padding\n",
        "4. Adapt and preserve the labels to match BERT's word piece tokenization\n",
        "5. Create attention masks and split data to train and test\n",
        "6. Use BERT-BASE-UNCASED pretrained model with the help of BERT Tokenizer\n",
        "7. Finetune all model layers for about 20 epochs\n",
        "8. Develop classification report\n",
        "9. Testing on Provided Validation Data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OqxXUy0yVG4G",
        "colab_type": "code",
        "outputId": "4862660a-c3f9-4759-a788-62319fd1f84e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "import os\n",
        "os.getcwd()\n",
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "#IF YOU ARE CLONING FOR THE FIRST TIME in colab please uncomment all the below 5 lines of code. \n",
        "#Put git user name and password in appropriate places\n",
        "\n",
        "root_path = 'gdrive/My Drive/Colab Notebooks/' #change dir to folder where you want to clone\n",
        "os.chdir('/content/' + root_path)\n",
        "##!git clone https://github.com/selfishhari/rental_meta_extraction.git\n",
        "\n",
        "root_path = 'rental_meta_extraction/' #change dir to your project folder's src\n",
        "\n",
        "os.chdir(root_path)\n",
        "\n",
        "!git branch -r | grep -v '\\->' | while read remote; do git branch --track \"${remote#origin/}\" \"$remote\"; done\n",
        "\n",
        "os.getcwd()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "fatal: A branch named 'master' already exists.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/gdrive/My Drive/Colab Notebooks/rental_meta_extraction'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JU8tJPmfbUoW",
        "colab_type": "code",
        "outputId": "a20c990a-e413-44f1-a130-f8e0ec3cbe65",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        }
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.11.0)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: tokenizers==0.7.0 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.4)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.12.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.15.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_NHgRUBWa1u",
        "colab_type": "code",
        "outputId": "1eb82773-aab3-41e0-d455-5135f6582479",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm, trange\n",
        "import os, sys\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import BertTokenizer, BertConfig\n",
        "\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "torch.__version__"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.5.0+cu101'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qj5dzs44Vado",
        "colab_type": "code",
        "outputId": "e6b21229-7dc1-456e-b0b3-1239ed76ab0f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        }
      },
      "source": [
        "annotated_data = pd.read_pickle(\"data/02_processed/annotated_df_v1.pickle\")\n",
        "\n",
        "annotated_data.sample(5)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>fname</th>\n",
              "      <th>paras</th>\n",
              "      <th>para_windows</th>\n",
              "      <th>para_windows_proc</th>\n",
              "      <th>annotations</th>\n",
              "      <th>annot_count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>62126501-Rental-Agreement</td>\n",
              "      <td>This Rental Agreement is made and executed on ...</td>\n",
              "      <td>RENTAL AGREEMENT This Rental Agreement is made...</td>\n",
              "      <td>rental agreement this rental agreement is made...</td>\n",
              "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>47854715-RENTAL-AGREEMENT</td>\n",
              "      <td>The lessor or their authorized agents shall be...</td>\n",
              "      <td>The said deposit shall be refundable to the le...</td>\n",
              "      <td>the said deposit shall be refundable to the le...</td>\n",
              "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>116950326-December-2012-Rental-Agreement</td>\n",
              "      <td>Megan’s Law</td>\n",
              "      <td>Fill out a) or b) as it appliesoto youf situat...</td>\n",
              "      <td>fill out a) or b) as it appliesoto youf situat...</td>\n",
              "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>323828497-Rental-Agreement-Micky</td>\n",
              "      <td>VEHICLES &amp; GARAGE USE:</td>\n",
              "      <td>ALTERATIONS: Tenant shall make no alterations,...</td>\n",
              "      <td>alterations: tenant shall make no alterations,...</td>\n",
              "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>6683129-House-Rental-Contract-Geraldine-Galina...</td>\n",
              "      <td>Surname, Name:</td>\n",
              "      <td>SUBLETTING: Without first requesting permissio...</td>\n",
              "      <td>subletting: without first requesting permissio...</td>\n",
              "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                fname  ... annot_count\n",
              "1                         62126501-Rental-Agreement    ...           1\n",
              "9                         47854715-RENTAL-AGREEMENT    ...           1\n",
              "30         116950326-December-2012-Rental-Agreement    ...           1\n",
              "36                 323828497-Rental-Agreement-Micky    ...           1\n",
              "19  6683129-House-Rental-Contract-Geraldine-Galina...  ...           1\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PtNFGcy5jbLx",
        "colab_type": "code",
        "outputId": "397e7dee-176e-45cd-f1e5-1e6937b3812a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "print(\"size of sentences with annotations\", annotated_data.loc[annotated_data.annot_count != 1, :].shape)\n",
        "\n",
        "print(\"size of sentences without any annotations\", annotated_data.loc[annotated_data.annot_count == 1, :].shape)\n",
        "\n",
        "print(\"Severely unbalanced. Hence will resample\")\n",
        "\n",
        "na = annotated_data.loc[annotated_data.annot_count == 1, :].sample(303)\n",
        "\n",
        "ad = annotated_data.loc[annotated_data.annot_count != 1, :]\n",
        "\n",
        "annotated_data = pd.concat([ad, na], ignore_index=True)\n",
        "\n",
        "print(\"size of sentences with annotations after undersampling no annotations\", annotated_data.loc[annotated_data.annot_count != 1, :].shape)\n",
        "\n",
        "print(\"size of sentences without any annotations after undersampling\", annotated_data.loc[annotated_data.annot_count == 1, :].shape)\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "size of sentences with annotations (303, 6)\n",
            "size of sentences without any annotations (1235, 6)\n",
            "Severely unbalanced. Hence will resample\n",
            "size of sentences with annotations after undersampling no annotations (303, 6)\n",
            "size of sentences without any annotations after undersampling (303, 6)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "is-yNrrnXKj3",
        "colab_type": "code",
        "outputId": "373f0e55-5822-49a7-b821-aedf300d5716",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "annotated_data[\"para_windows_proc\"] = annotated_data.para_windows_proc.apply(lambda x: x.replace(\".\", \" \"))\n",
        "\n",
        "annotated_data[\"para_windows_proc\"] = annotated_data.para_windows_proc.apply(lambda x: x.replace(\",\", \" \"))\n",
        "\n",
        "annotated_data[\"ann_length\"] = annotated_data[\"annotations\"].apply(lambda x: len(x))\n",
        "\n",
        "annotated_data[\"text_length\"] = annotated_data[\"para_windows_proc\"].apply(lambda x: len(x.split()))\n",
        "\n",
        "(~(annotated_data[\"text_length\"] == annotated_data[\"ann_length\"])).sum()\n",
        "\n",
        "print(\"all annotations and text lengths are now matching\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "all annotations and text lengths are now matching\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXAzC0kJYjKd",
        "colab_type": "code",
        "outputId": "925136d3-3794-4d91-c9a1-fd99ce5debec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "sentences = annotated_data[\"para_windows_proc\"].apply(lambda x: x.split()).tolist()\n",
        "\n",
        "labels = annotated_data[\"annotations\"].tolist()\n",
        "\n",
        "print(annotated_data.shape, len(sentences), len(labels))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(606, 8) 606 606\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xT4ULOmlaOz5",
        "colab_type": "code",
        "outputId": "393de6d9-bc08-440f-e55f-07b6e7a08cdb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "annotated_data[\"text_length\"].quantile([0.5, 0.7, 0.8, 0.9, 0.99, 1])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.50     59.00\n",
              "0.70     82.50\n",
              "0.80     99.00\n",
              "0.90    134.50\n",
              "0.99    233.35\n",
              "1.00    350.00\n",
              "Name: text_length, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7Srf2cQap1v",
        "colab_type": "text"
      },
      "source": [
        "Since 90% of text length is less than 141, I am going to choose **140** as initial **MAX_LEN**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TEz5JKC-aC34",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_LEN = 140\n",
        "bs = 64\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fONIPmMa8-J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "n_gpu = torch.cuda.device_count()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5WsBmKia9dR",
        "colab_type": "code",
        "outputId": "1b00cf49-04ef-4013-c45c-b821d33a5a49",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "torch.cuda.get_device_name(0)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Tesla P100-PCIE-16GB'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I2o_eAQnbFrU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBJwsWTRbkwL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize_and_preserve_labels(sentence, text_labels):\n",
        "    tokenized_sentence = []\n",
        "    labels = []\n",
        "\n",
        "    for word, label in zip(sentence, text_labels):\n",
        "\n",
        "        # Tokenize the word and count # of subwords the word is broken into\n",
        "        tokenized_word = tokenizer.tokenize(word)\n",
        "        n_subwords = len(tokenized_word)\n",
        "\n",
        "        # Add the tokenized word to the final tokenized word list\n",
        "        tokenized_sentence.extend(tokenized_word)\n",
        "\n",
        "        # Add the same label to the new list of labels `n_subwords` times\n",
        "        labels.extend([label] * n_subwords)\n",
        "\n",
        "    return tokenized_sentence, labels\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rqng4zYebpEF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenized_texts_and_labels = [\n",
        "    tokenize_and_preserve_labels(sent, labs)\n",
        "    for sent, labs in zip(sentences, labels)\n",
        "]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LrDRxTowb3Gi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenized_texts = [token_label_pair[0] for token_label_pair in tokenized_texts_and_labels]\n",
        "\n",
        "labels = [token_label_pair[1] for token_label_pair in tokenized_texts_and_labels]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FoR-ZytCcJOs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],\n",
        "                          maxlen=MAX_LEN, dtype=\"long\", value=0.0,\n",
        "                          truncating=\"post\", padding=\"post\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1iRl5rriwi8",
        "colab_type": "code",
        "outputId": "14602661-8c61-42ff-bc09-2f94f94ef886",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tag_values = [\"O\", \"B-P1\", \"I-P1\", \"B-P2\", \"I-P2\", \"PAD\"]\n",
        "\n",
        "tag2idx = {t: i for i, t in enumerate(tag_values)}\n",
        "\n",
        "tag2idx"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'B-P1': 1, 'B-P2': 3, 'I-P1': 2, 'I-P2': 4, 'O': 0, 'PAD': 5}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUedG_ytcPRB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tags = pad_sequences([[tag2idx.get(l) for l in lab] for lab in labels],\n",
        "                     maxlen=MAX_LEN, value=tag2idx[\"PAD\"], padding=\"post\",\n",
        "                     dtype=\"long\", truncating=\"post\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fP4J4l2fs2E",
        "colab_type": "code",
        "outputId": "d998fda7-ea42-4ac5-e865-39fc6c1efa34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "tags"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 0, ..., 5, 5, 5],\n",
              "       [0, 0, 0, ..., 5, 5, 5],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       ...,\n",
              "       [0, 0, 0, ..., 5, 5, 5],\n",
              "       [0, 0, 0, ..., 5, 5, 5],\n",
              "       [0, 0, 0, ..., 5, 5, 5]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "suQFttz_fvtn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "attention_masks = [[float(i != 0.0) for i in ii] for ii in input_ids]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bTs-TAP5fxqM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tr_inputs, val_inputs, tr_tags, val_tags = train_test_split(input_ids, tags,\n",
        "                                                            random_state=2019, test_size=0.1)\n",
        "\n",
        "tr_masks, val_masks, _, _ = train_test_split(attention_masks, input_ids,\n",
        "                                             random_state=2019, test_size=0.1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UnDQuJYkf2yO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tr_inputs = torch.tensor(tr_inputs)\n",
        "\n",
        "val_inputs = torch.tensor(val_inputs)\n",
        "\n",
        "tr_tags = torch.tensor(tr_tags)\n",
        "\n",
        "val_tags = torch.tensor(val_tags)\n",
        "\n",
        "tr_masks = torch.tensor(tr_masks)\n",
        "\n",
        "val_masks = torch.tensor(val_masks)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_nTQlb0f4wT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = TensorDataset(tr_inputs, tr_masks, tr_tags)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=bs)\n",
        "\n",
        "valid_data = TensorDataset(val_inputs, val_masks, val_tags)\n",
        "valid_sampler = SequentialSampler(valid_data)\n",
        "valid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=bs)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7vHUUGjIf6tj",
        "colab_type": "code",
        "outputId": "3bacc318-9c5a-4a22-a877-0412cb2557fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import transformers\n",
        "from transformers import BertForTokenClassification, AdamW\n",
        "\n",
        "transformers.__version__\n",
        "\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.11.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jgwuwDKjf8cF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = BertForTokenClassification.from_pretrained(\n",
        "    \"bert-base-uncased\",\n",
        "    num_labels=len(tag2idx),\n",
        "    output_attentions = False,\n",
        "    output_hidden_states = False\n",
        ")\n",
        "model.cuda();"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7y_ryZKgAxm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "FULL_FINETUNING = True\n",
        "\n",
        "if FULL_FINETUNING:\n",
        "    param_optimizer = list(model.named_parameters())\n",
        "    no_decay = ['bias', 'gamma', 'beta']\n",
        "    optimizer_grouped_parameters = [\n",
        "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "         'weight_decay_rate': 0.01},\n",
        "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
        "         'weight_decay_rate': 0.0}\n",
        "    ]\n",
        "else:\n",
        "    param_optimizer = list(model.classifier.named_parameters())\n",
        "    optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]\n",
        "\n",
        "optimizer = AdamW(\n",
        "    optimizer_grouped_parameters,\n",
        "    lr=3e-5,\n",
        "    eps=1e-8\n",
        ")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2di0lu6gIgL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "epochs = 25\n",
        "max_grad_norm = 1.0\n",
        "\n",
        "# Total number of training steps is number of batches * number of epochs.\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=total_steps\n",
        ")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-TFr09KgLtf",
        "colab_type": "code",
        "outputId": "6d4db9fb-f8b2-4e40-d431-d153f32b66e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        }
      },
      "source": [
        "!pip install seqeval\n",
        "!nvidia-smi"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: seqeval in /usr/local/lib/python3.6/dist-packages (0.0.12)\n",
            "Requirement already satisfied: Keras>=2.2.4 in /usr/local/lib/python3.6/dist-packages (from seqeval) (2.3.1)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from seqeval) (1.18.4)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (1.0.8)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (1.4.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (1.1.2)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (1.12.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (2.10.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (3.13)\n",
            "Mon Jun  8 03:36:08 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 440.82       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   57C    P0    35W / 250W |   1267MiB / 16280MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PxfYrpSagNdm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from seqeval.metrics import f1_score, accuracy_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oHiYCMIUgQPc",
        "colab_type": "code",
        "outputId": "f3b3a547-7218-4871-d33e-e550b30659fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "## Store the average loss after each epoch so we can plot them.\n",
        "loss_values, validation_loss_values = [], []\n",
        "\n",
        "for _ in trange(epochs, desc=\"Epoch\"):\n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "    # Perform one full pass over the training set.\n",
        "\n",
        "    # Put the model into training mode.\n",
        "    model.train()\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_loss = 0\n",
        "\n",
        "    # Training loop\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        # add batch to gpu\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        # Always clear any previously calculated gradients before performing a backward pass.\n",
        "        model.zero_grad()\n",
        "        # forward pass\n",
        "        # This will return the loss (rather than the model output)\n",
        "        # because we have provided the `labels`.\n",
        "        outputs = model(b_input_ids, token_type_ids=None,\n",
        "                        attention_mask=b_input_mask, labels=b_labels)\n",
        "        # get the loss\n",
        "        loss = outputs[0]\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "        # track train loss\n",
        "        total_loss += loss.item()\n",
        "        # Clip the norm of the gradient\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)\n",
        "        # update parameters\n",
        "        optimizer.step()\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over the training data.\n",
        "    avg_train_loss = total_loss / len(train_dataloader)\n",
        "    print(\"Average train loss: {}\".format(avg_train_loss))\n",
        "\n",
        "    # Store the loss value for plotting the learning curve.\n",
        "    loss_values.append(avg_train_loss)\n",
        "\n",
        "\n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "    # Put the model into evaluation mode\n",
        "    model.eval()\n",
        "    # Reset the validation loss for this epoch.\n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\n",
        "    predictions , true_labels = [], []\n",
        "    for batch in valid_dataloader:\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "        # Telling the model not to compute or store gradients,\n",
        "        # saving memory and speeding up validation\n",
        "        with torch.no_grad():\n",
        "            # Forward pass, calculate logit predictions.\n",
        "            # This will return the logits rather than the loss because we have not provided labels.\n",
        "            outputs = model(b_input_ids, token_type_ids=None,\n",
        "                            attention_mask=b_input_mask, labels=b_labels)\n",
        "        # Move logits and labels to CPU\n",
        "        logits = outputs[1].detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "        # Calculate the accuracy for this batch of test sentences.\n",
        "        eval_loss += outputs[0].mean().item()\n",
        "        predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n",
        "        true_labels.extend(label_ids)\n",
        "\n",
        "    eval_loss = eval_loss / len(valid_dataloader)\n",
        "    validation_loss_values.append(eval_loss)\n",
        "    print(\"Validation loss: {}\".format(eval_loss))\n",
        "    pred_tags = [tag_values[p_i] for p, l in zip(predictions, true_labels)\n",
        "                                 for p_i, l_i in zip(p, l) if tag_values[l_i] != \"PAD\"]\n",
        "    valid_tags = [tag_values[l_i] for l in true_labels\n",
        "                                  for l_i in l if tag_values[l_i] != \"PAD\"]\n",
        "    print(\"Validation Accuracy: {}\".format(accuracy_score(pred_tags, valid_tags)))\n",
        "    print(\"Validation F1-Score: {}\".format(f1_score(pred_tags, valid_tags)))\n",
        "    print()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:   0%|          | 0/25 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average train loss: 0.43401476244131726\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:   4%|▍         | 1/25 [00:06<02:46,  6.95s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss: 0.1876133680343628\n",
            "Validation Accuracy: 0.9667355371900826\n",
            "Validation F1-Score: 0\n",
            "\n",
            "Average train loss: 0.15541490995221668\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:   8%|▊         | 2/25 [00:13<02:39,  6.93s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss: 0.16871924698352814\n",
            "Validation Accuracy: 0.9667355371900826\n",
            "Validation F1-Score: 0\n",
            "\n",
            "Average train loss: 0.13554130329026115\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  12%|█▏        | 3/25 [00:20<02:32,  6.92s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss: 0.11841312050819397\n",
            "Validation Accuracy: 0.9667355371900826\n",
            "Validation F1-Score: 0\n",
            "\n",
            "Average train loss: 0.0963006400399738\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  16%|█▌        | 4/25 [00:27<02:25,  6.91s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss: 0.08685363829135895\n",
            "Validation Accuracy: 0.9725206611570248\n",
            "Validation F1-Score: 0.4097560975609757\n",
            "\n",
            "Average train loss: 0.06808476522564888\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  20%|██        | 5/25 [00:34<02:18,  6.91s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss: 0.061242904514074326\n",
            "Validation Accuracy: 0.9816115702479339\n",
            "Validation F1-Score: 0.5581395348837209\n",
            "\n",
            "Average train loss: 0.051277068754037224\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  24%|██▍       | 6/25 [00:41<02:11,  6.91s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss: 0.047659922391176224\n",
            "Validation Accuracy: 0.9855371900826446\n",
            "Validation F1-Score: 0.633587786259542\n",
            "\n",
            "Average train loss: 0.040366317249006696\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  28%|██▊       | 7/25 [00:48<02:04,  6.90s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss: 0.03937966376543045\n",
            "Validation Accuracy: 0.9857438016528925\n",
            "Validation F1-Score: 0.6080586080586081\n",
            "\n",
            "Average train loss: 0.031646628553668656\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  32%|███▏      | 8/25 [00:55<01:57,  6.90s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss: 0.03559859097003937\n",
            "Validation Accuracy: 0.9880165289256199\n",
            "Validation F1-Score: 0.6466165413533834\n",
            "\n",
            "Average train loss: 0.025918257526225515\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  36%|███▌      | 9/25 [01:02<01:50,  6.90s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss: 0.029571710154414177\n",
            "Validation Accuracy: 0.9913223140495868\n",
            "Validation F1-Score: 0.6942148760330579\n",
            "\n",
            "Average train loss: 0.022679692341221705\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  40%|████      | 10/25 [01:09<01:43,  6.89s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss: 0.02687137760221958\n",
            "Validation Accuracy: 0.9923553719008265\n",
            "Validation F1-Score: 0.7563025210084032\n",
            "\n",
            "Average train loss: 0.018143709955943957\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  44%|████▍     | 11/25 [01:15<01:36,  6.89s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss: 0.026477549225091934\n",
            "Validation Accuracy: 0.9911157024793389\n",
            "Validation F1-Score: 0.7438016528925621\n",
            "\n",
            "Average train loss: 0.014570386666390631\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  48%|████▊     | 12/25 [01:22<01:29,  6.89s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss: 0.02357451058924198\n",
            "Validation Accuracy: 0.9921487603305785\n",
            "Validation F1-Score: 0.7634854771784233\n",
            "\n",
            "Average train loss: 0.0120726992479629\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  52%|█████▏    | 13/25 [01:29<01:22,  6.89s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss: 0.019701460376381874\n",
            "Validation Accuracy: 0.993801652892562\n",
            "Validation F1-Score: 0.8068669527896997\n",
            "\n",
            "Average train loss: 0.010930041575597392\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  56%|█████▌    | 14/25 [01:36<01:15,  6.90s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss: 0.017978090792894363\n",
            "Validation Accuracy: 0.9940082644628099\n",
            "Validation F1-Score: 0.8240343347639485\n",
            "\n",
            "Average train loss: 0.009314440863413943\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  60%|██████    | 15/25 [01:43<01:08,  6.89s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss: 0.01646634377539158\n",
            "Validation Accuracy: 0.9950413223140496\n",
            "Validation F1-Score: 0.8558951965065502\n",
            "\n",
            "Average train loss: 0.009156979723936982\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  64%|██████▍   | 16/25 [01:50<01:02,  6.90s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss: 0.015236816368997097\n",
            "Validation Accuracy: 0.9960743801652893\n",
            "Validation F1-Score: 0.8669527896995708\n",
            "\n",
            "Average train loss: 0.007832939063923227\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  68%|██████▊   | 17/25 [01:57<00:55,  6.90s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss: 0.014242805540561676\n",
            "Validation Accuracy: 0.9958677685950413\n",
            "Validation F1-Score: 0.8608695652173913\n",
            "\n",
            "Average train loss: 0.006762658452822102\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  72%|███████▏  | 18/25 [02:04<00:48,  6.90s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss: 0.014294332824647427\n",
            "Validation Accuracy: 0.9960743801652893\n",
            "Validation F1-Score: 0.8793103448275862\n",
            "\n",
            "Average train loss: 0.006126289152436786\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  76%|███████▌  | 19/25 [02:11<00:41,  6.90s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss: 0.012969590723514557\n",
            "Validation Accuracy: 0.9962809917355372\n",
            "Validation F1-Score: 0.9004329004329005\n",
            "\n",
            "Average train loss: 0.006314619754751523\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  80%|████████  | 20/25 [02:18<00:34,  6.91s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss: 0.013515891507267952\n",
            "Validation Accuracy: 0.9960743801652893\n",
            "Validation F1-Score: 0.8755364806866952\n",
            "\n",
            "Average train loss: 0.005694108497765329\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  84%|████████▍ | 21/25 [02:24<00:27,  6.91s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss: 0.010902205482125282\n",
            "Validation Accuracy: 0.9973140495867768\n",
            "Validation F1-Score: 0.9391304347826088\n",
            "\n",
            "Average train loss: 0.004636222781199548\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  88%|████████▊ | 22/25 [02:31<00:20,  6.90s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss: 0.010930899530649185\n",
            "Validation Accuracy: 0.996900826446281\n",
            "Validation F1-Score: 0.9264069264069263\n",
            "\n",
            "Average train loss: 0.004883849915737907\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  92%|█████████▏| 23/25 [02:38<00:13,  6.90s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss: 0.011430526152253151\n",
            "Validation Accuracy: 0.996694214876033\n",
            "Validation F1-Score: 0.9137931034482759\n",
            "\n",
            "Average train loss: 0.004536408103174633\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  96%|█████████▌| 24/25 [02:45<00:06,  6.89s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss: 0.011072388850152493\n",
            "Validation Accuracy: 0.9971074380165289\n",
            "Validation F1-Score: 0.9396551724137931\n",
            "\n",
            "Average train loss: 0.004360001094432341\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 100%|██████████| 25/25 [02:52<00:00,  6.90s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss: 0.010879842564463615\n",
            "Validation Accuracy: 0.9971074380165289\n",
            "Validation F1-Score: 0.9396551724137931\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2qaA8B9gUN9",
        "colab_type": "code",
        "outputId": "582b61d9-a6d7-401f-b34f-1bc46b8c148f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "from seqeval.metrics import precision_score, classification_report\n",
        "\n",
        "\n",
        "print(classification_report(valid_tags, pred_tags))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "           precision    recall  f1-score   support\n",
            "\n",
            "       P2       0.88      0.94      0.91        32\n",
            "       P1       0.93      0.98      0.95        81\n",
            "\n",
            "micro avg       0.92      0.96      0.94       113\n",
            "macro avg       0.92      0.96      0.94       113\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7M_6U_4hZQR",
        "colab_type": "code",
        "outputId": "d8a132ff-fa83-4c93-acae-c9f5636b501c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mon Jun  8 03:39:03 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 440.82       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   69C    P0    44W / 250W |  10559MiB / 16280MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvTrY-4z0aMH",
        "colab_type": "text"
      },
      "source": [
        "**Testing on validation set**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDFM-qf20YNB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "4ee6b83d-635b-48dd-b37a-7dab8c4dd4ac"
      },
      "source": [
        "annotated_data_valid = pd.read_pickle(\"data/02_processed/annotated_df_valid.pickle\")\n",
        "\n",
        "annotated_data_valid[\"para_windows_proc\"] = annotated_data_valid.para_windows_proc.apply(lambda x: x.replace(\".\", \" \"))\n",
        "\n",
        "annotated_data_valid[\"para_windows_proc\"] = annotated_data_valid.para_windows_proc.apply(lambda x: x.replace(\",\", \" \"))\n",
        "\n",
        "annotated_data_valid[\"ann_length\"] = annotated_data_valid[\"annotations\"].apply(lambda x: len(x))\n",
        "\n",
        "annotated_data_valid[\"text_length\"] = annotated_data_valid[\"para_windows_proc\"].apply(lambda x: len(x.split()))\n",
        "\n",
        "print((~(annotated_data_valid[\"text_length\"] == annotated_data_valid[\"ann_length\"])).sum())\n",
        "\n",
        "print(\"all annotations and text lengths are now matching\")\n",
        "\n",
        "sentences_valid = annotated_data_valid[\"para_windows_proc\"].apply(lambda x: x.split()).tolist()\n",
        "\n",
        "labels_valid = annotated_data_valid[\"annotations\"].tolist()\n",
        "\n",
        "print(annotated_data_valid.shape, len(sentences_valid), len(labels_valid))\n",
        "\n",
        "tokenized_texts_and_labels_valid = [\n",
        "    tokenize_and_preserve_labels(sent, labs)\n",
        "    for sent, labs in zip(sentences_valid, labels_valid)\n",
        "]\n",
        "\n",
        "tokenized_texts_valid = [token_label_pair[0] for token_label_pair in tokenized_texts_and_labels_valid]\n",
        "\n",
        "labels_valid = [token_label_pair[1] for token_label_pair in tokenized_texts_and_labels_valid]\n",
        "\n",
        "input_ids_valid = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts_valid],\n",
        "                          maxlen=MAX_LEN, dtype=\"long\", value=0.0,\n",
        "                          truncating=\"post\", padding=\"post\")\n",
        "\n",
        "tags_valid = pad_sequences([[tag2idx.get(l) for l in lab] for lab in labels_valid],\n",
        "                     maxlen=MAX_LEN, value=tag2idx[\"PAD\"], padding=\"post\",\n",
        "                     dtype=\"long\", truncating=\"post\")\n",
        "attention_masks_valid = [[float(i != 0.0) for i in ii] for ii in input_ids_valid]\n",
        "\n",
        "vl_inputs = torch.tensor(input_ids_valid)\n",
        "\n",
        "vl_tags = torch.tensor(tags_valid)\n",
        "\n",
        "vl_masks = torch.tensor(attention_masks_valid)\n",
        "\n",
        "vl_data = TensorDataset(vl_inputs, vl_masks, vl_tags)\n",
        "vl_sampler = SequentialSampler(vl_data)\n",
        "vl_dataloader = DataLoader(vl_data, sampler=vl_sampler, batch_size=bs)\n"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "all annotations and text lengths are now matching\n",
            "(252, 8) 252 252\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5R5Xl2eh28Jl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "d6a06837-918b-4faa-8175-39744266e74b"
      },
      "source": [
        "model.eval()\n",
        "\n",
        "\n",
        "eval_loss, eval_accuracy = 0, 0\n",
        "nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "predictions , true_labels = [], []\n",
        "\n",
        "for batch in vl_dataloader:\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "    # Telling the model not to compute or store gradients,\n",
        "    # saving memory and speeding up validation\n",
        "    with torch.no_grad():\n",
        "        # Forward pass, calculate logit predictions.\n",
        "        # This will return the logits rather than the loss because we have not provided labels.\n",
        "        outputs = model(b_input_ids, token_type_ids=None,\n",
        "                        attention_mask=b_input_mask, labels=b_labels)\n",
        "    # Move logits and labels to CPU\n",
        "    logits = outputs[1].detach().cpu().numpy()\n",
        "    label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "    # Calculate the accuracy for this batch of test sentences.\n",
        "    eval_loss += outputs[0].mean().item()\n",
        "    predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n",
        "    true_labels.extend(label_ids)\n",
        "\n",
        "eval_loss = eval_loss / len(vl_dataloader)\n",
        "\n",
        "print(\"Validation loss: {}\".format(eval_loss))\n",
        "\n",
        "pred_tags_vl = [tag_values[p_i] for p, l in zip(predictions, true_labels)\n",
        "                              for p_i, l_i in zip(p, l) if tag_values[l_i] != \"PAD\"]\n",
        "\n",
        "valid_tags_vl = [tag_values[l_i] for l in true_labels\n",
        "                              for l_i in l if tag_values[l_i] != \"PAD\"]\n",
        "\n",
        "print(\"Validation Accuracy: {}\".format(accuracy_score(pred_tags_vl, valid_tags_vl)))\n",
        "print(\"Validation F1-Score: {}\".format(f1_score(pred_tags_vl, valid_tags_vl)))\n",
        "print()"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Validation loss: 0.02437988921883516\n",
            "Validation Accuracy: 0.9963228957196208\n",
            "Validation F1-Score: 0.8123249299719887\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "heBD_gkO3iOZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "e62fe89d-3175-4ed7-b9d3-296998471994"
      },
      "source": [
        "from seqeval.metrics import precision_score, classification_report\n",
        "\n",
        "#10 epochs\n",
        "print(classification_report(valid_tags_vl, pred_tags_vl))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "           precision    recall  f1-score   support\n",
            "\n",
            "       P1       0.82      0.88      0.85        93\n",
            "       P2       0.72      0.82      0.77        77\n",
            "\n",
            "micro avg       0.78      0.85      0.81       170\n",
            "macro avg       0.78      0.85      0.81       170\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}